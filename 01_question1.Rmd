---
title: "01_question1"
author: '44'
date: "2022-06-02"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q1

A trial was performed to assess the impact of an intervention on the
number of times a sedentary individual engages in 10 minutes of
sustained moderate-to-vigorous physically active over a week.

The study recruited participants in matched pairs (matching on age),
assigning the intervention to one individual within each age pair (n
total pairs).

Denote each pair using the subscript $i$ and let the number of active
bouts for the person in the $i$th pair be denoted as $Y_{i1}$, $Y_{i2}$,
with $Y_{i1}$ corresponding to individual who received the intervention.

Let $X_i$ be the age for individuals in the $i$th pair.

It is hypothesized that

$$
\begin{split}
Y_{i1}|X_i \sim Poisson(\gamma \beta(X_i))\\
Y_{i2}|X_i \sim Poisson(\beta (X_i))
\end{split}
$$

where $\beta(·)$ is an unspecified function of $X_i$ (age).

The investigators of the current study are uninterested in $\beta(·)$.

Assume that the distribution of age in the population follows some
distribution $f_X(x)$.

If the number of pairs ($n$) observed is large and $X$ is observed
densely relative to the shape of $\beta(·)$ then $\gamma, \beta$ may be
estimable using the likelihood of the data.

However, if n is small or $X$ is sparsely observed, estimating both
$\gamma$ and $\beta$ may not be possible. This question involves
deriving a conditional likelihood where you condition out the nuisance
parameter(s) $\beta(·)$ so that $\gamma$ is estimable in such scenarios.

You will implement a short simulation study and apply your method to
data from the study referenced above.

## (a) 

In the first part of this problem,
we will condition out nuisance parameters ($\beta$) to obtain a
likelihood for $\gamma$ which depends only on the observed data.

### i. 

What is the likelihood of
$[X_i, Y_{i1}, Y_{i2}]^T | \gamma, \beta(·)$? Your answer will involve
the marginal distribution of $X$, $f_X(x)$.


$$
\begin{split}
L(\gamma, \beta(·) ; \pmb X, \pmb Y_{1}, \pmb Y_{2}) & =
\prod_{i=1}^n L(\gamma, \beta(·) ; X_i, Y_{i1}, Y_{i2}) \\
& = \prod_{i=1}^n f_{Y_1, Y_2}(y_{i1}, y_{i2} | x_i; \gamma, \beta(·)) \times f_{X}(x_i ; \gamma, \beta(.)) \\
& = \prod_{i=1}^n f_{Y_{1}}(y_{i1}|x_i; \gamma, \beta(.)) \times f_{Y_{2}}(y_{2}| x_i; \beta(.)) \times f_X(x_i) \\
& = \prod_{i=1}^n \frac {(\gamma \beta(x_i))^{y_{i1}} e^{-\gamma \beta(x_i)}} {y_{i1}!} 
\frac {( \beta(x_i))^{y_{i2}} e^{-\beta(x_i)}} {y_{i2}!} f_X(x_i)\\
& = \prod_{i=1}^n \frac {\gamma^{y_{i1}}\beta (x_i)^{y_{i1} + y_{i2}} e^{-(\gamma \beta(x_i) + \beta(x_i))}}
{y_{i1}! y_{i2}!} f_X(x_i)
\end{split}
$$

### ii. 

What is the likelihood of $Y_{i1} + Y_{i2}|Xi, \theta$? Identify the corresponding distribution.

The summation of two Poisson distribution is also a Poisson distribution. 
$Y_{i1} + Y_{i2} |Xi \sim Poisson(\gamma \beta(X_i) + \beta(X_i))$


$$
\begin{split}
& \begin{cases}
U = Y_1 + Y_{2} \\
V = Y_{1}\\
\end{cases}\\
& Y_{1}, Y_{2} \geq 0; 0 \leq V \leq U\\
& \begin{cases}
Y_1 = V\\
Y_2 = {U - V}
\end{cases} \\
& J = 
\begin{bmatrix}
\frac {\partial u} {\partial y_1} & \frac {\partial u} {\partial y_2} \\
\frac {\partial v} {\partial y_1} & \frac {\partial v} {\partial y_2}
\end{bmatrix} =
\begin{bmatrix}
1 & 1 \\ 1 & 0
\end{bmatrix}= -1
\end{split}
$$

$$
\begin{split}
f_{U, V}(u, v|x; \gamma, \beta(.)) & = 
f_{Y_1, Y_2}(y_1 + y_2, y_1|x; \gamma, \beta(.)) \times |J| \\
& = \frac {\gamma^{v}\beta (x)^{u} e^{-(\gamma \beta(x) + \beta(x))}}
{v! (u - v)!} \\
for:\ 0 \leq V \leq U_{i}\\
f_{U}(u | x; \gamma, \beta(.)) & =
\int f_{U, V}(u, v|x; \gamma, \beta(.)) dv \\
& = \int_{0}^u \frac {\gamma^{v}\beta (x_i)^{u_i} e^{-(\gamma \beta(x_i) + \beta(x_i))}}
{v_i! (u_i - v_i)!} dv \\
& = \frac {(\gamma \beta(x_i) + \beta(x_i))^{u_{i}} e^{-\gamma \beta(x_i) - \beta(x_i)}} {u_{i}!} \\
U_i | X_i  \equiv  Y_{i1} + Y_{i2} |X_i & \sim Poisson(\gamma \beta(X_i) + \beta(X_i))
\end{split}
$$


### iii. 

What is the likelihood of $Y_{i1} | Y_{i1} + Y_{i2}, X_{i}, \theta$? Identify the corresponding distribution.

$$
\begin{split}
L(\theta; v | u, x) & = f_{V|U}(v | u, x; \theta)  = \frac {f_{U, V} (v , u| x; \theta)} {f_{U}(u | x; \theta)} \\
& = \frac {\gamma^{v}\beta (x_i)^{u_i} e^{-(\gamma \beta(x_i) + \beta(x_i))}}
{v_i! (u_i - v_i)!} \times \frac {u_{i}!} {(\gamma \beta(x_i) + \beta(x_i))^{u_{i}} e^{-\gamma \beta(x_i) - \beta(x_i)}} \\
& = \frac  {u_i!} {v_i! (u_i - v_i)!} \frac {\gamma^{v_i} \beta^{u_i}} {(\gamma\beta + \beta)^{u_i}} \\
& = \frac  {u_i!} {v_i! (u_i - v_i)!} \bigg(\frac {\gamma\beta} {\gamma \beta + \beta} \bigg)^{v_i} \bigg(\frac {\beta} {\gamma\beta + \beta}\bigg)^{u_i - v_i} \\
& = \frac  {u_i!} {v_i! (u_i - v_i)!} \bigg(1 - \frac {1} {\gamma + 1} \bigg)^{v_i} \bigg(\frac {1} {\gamma + 1}\bigg)^{u_i - v_i} 
\end{split}
$$


The distribution of $(V_{i} | U_i, X_{i}; \theta) \equiv (Y_{i1} | Y_{i1} + Y_{i2}, X_{i}; \theta)$ is $(V_{i} | U_i, X_{i}; \theta) \sim Binomial(U_i, \frac {1} {\gamma + 1})$, with sample size $U_i$ and $p = \frac {1} {\gamma + 1}$

The distribution for the parameter $\theta$ is $\frac {1} {1 + \gamma} \sim Beta(u_i - v_i + 1, v_i + 1)$

### iv. 

What is the maximum likelihood estimator of $\gamma$ obtained from the conditional likelihood you derived in iii?

If $\hat \theta$ is the MLE of $\theta$, then $g(\hat \theta)$ is the MLE of $g(\theta)$; $\gamma = g(\theta) = \theta^{-1} - 1$


$$
\begin{split}
L(\gamma; \pmb v | \pmb u, \pmb x) & = \prod_{i=1}^n L(\theta; v_i | u_i, x_i)\\
\log L(\gamma; \pmb v | \pmb u, \pmb x) & = \sum_{i=1}^n \log L(\theta; v_i | u_i, x_i)\\
& = \sum_{i=1} ^n \log\bigg(\frac  {u_i!} {v_i! (u_i - v_i)!}\bigg) +
\log \bigg(1 - \frac {1} {\gamma + 1} \bigg) \sum_{i=1}^n v_i  + 
\log \bigg(\frac {1} {\gamma + 1}\bigg)\sum_{i=1}^n ({u_i - v_i} )\\
\frac {\partial \log L(\gamma; \pmb v | \pmb u, \pmb x)} {\partial \gamma} & =
\frac {1} {\gamma (\gamma+1)} \sum_{i=1}^n v_i  - \frac {1} {(\gamma + 1)}\sum_{i=1}^n ({u_i - v_i} ) \stackrel {set}{=} 0\\
& \hat \gamma  \neq -1 \ or \ 0 \ \ \\
\hat \gamma & = \frac {\sum_{i=1}^n v_i} {\sum_{i=1}^n (u_i - v_i)} = \frac {\sum_{i=1}^n y_{i1}} {\sum_{i=1}^n y_{i2}} = \frac {\bar Y_1} {\bar Y_2}\\
\frac {\partial^2 \log L(\gamma; \pmb v | \pmb u, \pmb x)} {\partial \gamma^2} & =
- \frac {2 \gamma + 1} {\gamma^2 (\gamma+1)^2} \sum_{i=1}^n v_i  + \frac {1} {(\gamma + 1)^2}\sum_{i=1}^n ({u_i - v_i} ) \\
given \ \hat \gamma, \ & \frac {\partial^2 \log L(\gamma; \pmb v | \pmb u, \pmb x)} {\partial \gamma^2} \leq 0;\ \hat \gamma \ is \ MLE
\end{split}
$$


## (b) 

Propose a method for constructing an asymptotic 95% confidence interval for $\gamma$ based on your result from (a) iv using maximum likelihood theory (you may use the observed information here).

The MLE $\hat \gamma$ is asymptotically normal under regularity conditions. 

$$
\begin{split}
& \sqrt n (\hat \gamma - \gamma) \stackrel {L} {\rightarrow} Normal (0, I(\gamma)^{-1})\\
& Var[\hat \gamma] = I[\gamma] ^ {-1} = - \bigg(\frac {1} {n} \frac {\partial^2}  {\partial \gamma^2} \log L(\gamma; \pmb v | \pmb u, \pmb x)\bigg)^{-1}= \frac{(\sum_{i=1}^n u_i)^3 (\sum _{i=1}^n u_i-v_i)}{n (\sum_{i=1}^n v_i)^3} = \frac {(\bar Y_1 + \bar Y_2)^3 (\bar Y_2)} {\bar Y_1^3}\\
\end{split}
$$

Hence, an asymptotic 95% confidence interval for $\gamma$ can be calculated using $\hat \gamma \pm 1.96 se[\hat \gamma]$



## (c) 

A statistician colleague of yours suggests constructing of a confidence interval 
using simulation. Specifically, your colleague proposes the following procedure:

Fix $\alpha$, the type-I error rate. 


- Step 1 Choose candidate interval endpoints $\gamma^∗ = [\gamma_1^∗, \gamma_2^∗, ... , \gamma_S^∗]^{\top}$ which is dense over a range of plausible
endpoints for the confidence interval. 

- Step 2 For each $\gamma_s^∗ \in \gamma^∗$ 
  - Step 2a Simulate $y^s_{i1} | y_{i1} + y_{i2}$, $\gamma_s^∗$ using the distribution you derived in (a)iii.
  - Step 2b Calculate $\gamma_s^∗$ as the MLE you derived in (a)iv. 
  - Step 2c Repeat (1)-(2) many times (e.g. 2000).
  - Step 2d Let $I_s = 1 \Big(\hat \gamma \in \big[\hat F_{\gamma_s}^{-1} (\alpha/2), \hat F_{\gamma_s}^{-1} (1- \alpha/2)\big]\Big)$, where $\hat F_{\gamma_s}^{-1} (\alpha/2)$) and $\hat F_{\gamma_s}^{-1} (1- \alpha/2)$ are the $\alpha/2$ and $1 − \alpha/2$ percentiles
of $\gamma_s^∗$ obtained from Steps 2a-2c, respectively. 

- Step 3 Define a $1 − \alpha$% CI for $\gamma$ as $\big[{min}_{\{s: I_s =1\}} (\gamma_s^∗), {max}_{\{s:I_s =1\}} (\gamma_s^∗) \big] $ Argue that your colleague's algorithm is a valid approach for constructing a 95% confidence interval. 


It is the same as using a stochastic process to find the empirical distribution of $\gamma$. According to Donsker's theroem and Glivenko-Cantelli theorem, we know that the empirical process distribution convergence to the true distribution asymptotically. Hence we can use sampling to generate a valide confidence interval.


Write a function which implements this algorithm for arbitrary: 
- Type-I error $\alpha$ 
- Input vectors $\pmb y_1, \pmb y_2$
- Candidate interval endpoints $\gamma^∗$ 
- Number of simulated datasets created in step 2c

$(V_{i} | U_i, X_{i}; \theta) \sim Binomial(U_i, \frac {1} {\gamma + 1})$

$\hat \gamma = \frac {\sum_{i=1}^n y_{i1}} {\sum_{i=1}^n y_{i2}} = \frac {\bar Y_1} {\bar Y_2}$




```{r}
library(tidyverse)
s15 <- rnorm(n = 5000, mean = 2.2, sd = 0.163)
s100 <- rnorm(n = 5000, mean = 2.2, sd = 0.163)
ggplot() +
  geom_freqpoly(aes(s15), color = "orange") +
  geom_freqpoly(aes(s100), color = "red") +
  geom_vline(xintercept = 1,
             linetype="dashed", 
                color = "black", 
             size = 1) +
  ylab("gamma") + 
  theme_bw()
```



```{r}
load("bios_qexam_data_1.Rdata")

## function for question 1d
get_gamma <- function(y,
                      alpha = 0.05,
                      gamma_min = 0,
                      gamma_max = 5.5,
                      gamma_step = 0.01,
                      N = 2000) {
  y <- as.data.frame(y)
  ## sum of y1 y2
  y1 <- y[, 1]
  y2 <- y[, 2]
  y12 <- y1 + y2
  gamma_int <- seq(gamma_min, gamma_max, by = gamma_step)
  gamma_hat <- sum(y1) / sum(y2)
  ## emptry matrix to store the values
  gamma_mle <- matrix(NA, nrow = N, ncol = length(gamma_int))

  for (j in seq_along(1:N)) {
    for (i in seq_along(1:length(gamma_int))) {
      phat <- gamma_int[i] / (1 + gamma_int[i])
      ## simulate y1 | y1+y2 2000 times
      # ya3 <- map(y12, ~rbinom(n = 1, size = ., 
      #                         prob = phat)) %>% unlist()
      
      ya3 <- rbinom(length(y1),
                    size = unlist(y1 + y2),
                    prob = phat)
      gamma_mle[j, i] <- sum(ya3) / sum(y12 - ya3) 
    }
  }
  
  ## get F(0.25) and F(0.975)
  q025 <- apply(gamma_mle, 2, 
                quantile, probs = alpha / 2,
                na.rm = TRUE)

  q975 <- apply(gamma_mle, 2, 
                quantile, probs = 1 - alpha / 2, 
                na.rm = TRUE)
  ## check whether gamma_int is located inside F025 F975
  gamma_ind <- ifelse((gamma_hat > q025) & (gamma_hat < q975), 1, 0)
  ## find the smallest and largest gamma_int
  lower <- min(gamma_int[which(gamma_ind == 1)])
  upper <- max(gamma_int[which(gamma_ind == 1)])
  return(c(lower, upper))
}

get_gamma(y_ss100[[1]])

```

```{r eval=FALSE, include=TRUE}
## parallel programming
library(furrr)
library(parallel)
detectCores()

## use 6 cores
plan(multisession, workers = 6)
finall_5k_N15 <- future_map_dfc(y_ss15, ~get_gamma(y = .))
finall_5k_N100 <- future_map_dfc(y_ss100, ~get_gamma(y = .))

sum(ifelse(1 > finall_5k_N15[1,] & 1 < finall_5k_N15[2, ], 1, 0)) / 5000
sum(ifelse(1 > finall_5k_N100[1,] & 1 < finall_5k_N100[2, ], 1, 0))/ 5000

save(finall_5k_N15, finall_5k_N100, finall_per,
     file = "bios_qexam_question1.Rdata")
finall_5k_N15
hist(unlist(finall_5k_N15[1, ]))
finall_5k_N100 <- rnorm()
hist
```


```{r eval=FALSE, include=TRUE}
load("bios_qexam_question1.Rdata")
library(matrixStats)
rowMedians(as.matrix(finall_5k_N15))
rowMedians(as.matrix(finall_5k_N100))
```


## (d) 

Implement a fully reproducible simulation study which assesses the performance of the confidence intervals obtained in (b) and (c). Specifically, implement a simulation study as follows:

- $X ∼ Unif(0, 1)$,
- $\gamma = 2.2$,  
- $\beta(x) = 2e^{−2x}$,
- $n=15 \& n=100$, 
- $\gamma^∗$ is an equally spaced grid on $[0,5.5]$ with interval length of $0.01$ 
- Simulate $2000$ datasets in Step 2c of the algorithm from (c)

Estimate 95% confidence intervals using the approaches you derived in both (b) and (c) in 5000 simulated datasets and evaluate power for testing the null hypothesis $H_0 : \gamma = 1$. Compare approaches on coverage probability and
power. Summarize your results in 3-4 sentences as if for a statistical journal. Support your claims using 1-2 tables or figures. Provide well commented code which reproduces your simulation study and recreates your tables/figures.

The confidence interval for sample size 15 is [1.89, 3.11]

The confidence interval for sample size 100 is [1.95, 2.97]


## (e) 

Apply your methods to the dataset "MVPA intervention.csv", a data matrix which contains two columns, "Y1" and "Y2", corresponding to the number of 10 minute bouts of moderate-to-vigorous physical activity in matched pairs. Estimate $\gamma$ and construct 95% confidence intervals using both of the approaches you derived in (b) and (c). 

Write a brief, non-technical, summary of both methods as if communicating to a non-statistician, recommend and justify one approach, and conclude with an interpretation of the estimated coefficient and confidence interval.


```{r eval=FALSE, include=TRUE}
library(tidyverse)
y <- read_csv("data/MVPA_intervention.csv")
y1 <- y$Y1
y2 <- y$Y2
y1_bar <- mean(y1)
y2_bar <- mean(y2)
## mle 
(mean <- y1_bar / y2_bar)
## sd 
## use observed information
(gamma <- y1_bar / y2_bar)
Inof <- ((2 * gamma + 1) / (gamma^2 * (1 + gamma)^2) * sum(y1) - 1 / (gamma + 1)^2 * sum(y2)) / 15
(sd <- (1 / Inof) %>% sqrt()) / sqrt(15); sd
## use phat
sd0 <- sqrt((y1_bar + y2_bar) * y1_bar / y2_bar^3) / sqrt(15)
## 95% CI
mean - 1.96 * sd0
mean + 1.96 * sd0 

get_gamma(y = y, gamma_max = 15)
```


```{r eval=FALSE, include=TRUE}
## use permutation to get the confidence interval 
y_permu <- map(1:1000, ~sample_n(y, size = 15, replace = TRUE))

## parallel programming
library(furrr)
library(parallel)

## use 6 cores
plan(multisession, workers = 6)
finall_per <- future_map_dfc(y_permu, ~get_gamma(y = ., gamma_max = 10))
rowMedians(as.matrix(finall_per))
## [1] 1.67 9.95
## [1] 1.825 8.640

```

